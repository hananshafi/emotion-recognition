{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization, Conv2D, Activation, MaxPooling2D,MaxPooling2D,AveragePooling2D, Dense, GlobalAveragePooling2D\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout, Flatten, Concatenate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import keras_resnet.models\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('legend.csv',header = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack([cv2.imread('images\\\\'+str(img)) for img in df.iloc[5:905,1]]).astype(np.float)[:,:,:,np.newaxis]\n",
    "x_t = np.stack([cv2.imread('images\\\\'+str(img1)) for img1 in df.iloc[905:1205,1]]).astype(np.float)[:,:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.vstack(df.iloc[5:905,-1].values)\n",
    "y_t = np.vstack(df.iloc[905:1205,-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y.reshape(900,)\n",
    "y_test = y_t.reshape(300,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_Y = encoder.transform(y_train)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "y1 = np_utils.to_categorical(encoded_Y)\n",
    "encoder.fit(y_test)\n",
    "encoded_Y_t = encoder.transform(y_test)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "y2 = np_utils.to_categorical(encoded_Y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X / 255\n",
    "X_test = x_t / 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(900,350,350,3)\n",
    "X_test = X_test.reshape(300,350,350,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_2 (Batch (5, 350, 350, 3)          12        \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (5, 350, 350, 8)          104       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (5, 175, 175, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (5, 175, 175, 8)          264       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (5, 87, 87, 8)            0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (5, 87, 87, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (5, 87, 87, 16)           528       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (5, 43, 43, 16)           0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (5, 43, 43, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (5, 43, 43, 32)           2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (5, 21, 21, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (5, 21, 21, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (5, 14112)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (5, 50)                   705650    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (5, 6)                    306       \n",
      "=================================================================\n",
      "Total params: 708,944\n",
      "Trainable params: 708,938\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n",
      "Train on 810 samples, validate on 90 samples\n",
      "Epoch 1/20\n",
      "810/810 [==============================] - 32s 39ms/step - loss: 1.0491 - acc: 0.5926 - val_loss: 1.0402 - val_acc: 0.5889\n",
      "Epoch 2/20\n",
      "810/810 [==============================] - 15s 19ms/step - loss: 0.7981 - acc: 0.6827 - val_loss: 0.8649 - val_acc: 0.7778\n",
      "Epoch 3/20\n",
      "810/810 [==============================] - 15s 18ms/step - loss: 0.7013 - acc: 0.7284 - val_loss: 0.7474 - val_acc: 0.7889\n",
      "Epoch 4/20\n",
      "810/810 [==============================] - 15s 18ms/step - loss: 0.6562 - acc: 0.7407 - val_loss: 0.8594 - val_acc: 0.7667\n",
      "Epoch 5/20\n",
      "810/810 [==============================] - 15s 18ms/step - loss: 0.5652 - acc: 0.7914 - val_loss: 0.8092 - val_acc: 0.8333\n",
      "Epoch 6/20\n",
      "810/810 [==============================] - 15s 19ms/step - loss: 0.5154 - acc: 0.7926 - val_loss: 0.6672 - val_acc: 0.8222\n",
      "Epoch 7/20\n",
      "810/810 [==============================] - 15s 19ms/step - loss: 0.4407 - acc: 0.8185 - val_loss: 0.7006 - val_acc: 0.8333\n",
      "Epoch 8/20\n",
      "810/810 [==============================] - 15s 18ms/step - loss: 0.3730 - acc: 0.8457 - val_loss: 0.7189 - val_acc: 0.7667\n",
      "Epoch 9/20\n",
      "810/810 [==============================] - 14s 18ms/step - loss: 0.3030 - acc: 0.8938 - val_loss: 0.7178 - val_acc: 0.7111\n",
      "Epoch 10/20\n",
      "810/810 [==============================] - 14s 18ms/step - loss: 0.2667 - acc: 0.9012 - val_loss: 0.7102 - val_acc: 0.7333\n",
      "Epoch 11/20\n",
      "810/810 [==============================] - 14s 17ms/step - loss: 0.2940 - acc: 0.8840 - val_loss: 0.8047 - val_acc: 0.7333\n",
      "Epoch 12/20\n",
      "810/810 [==============================] - 15s 18ms/step - loss: 0.1940 - acc: 0.9235 - val_loss: 0.7729 - val_acc: 0.8000\n",
      "Epoch 13/20\n",
      "810/810 [==============================] - 13s 17ms/step - loss: 0.2410 - acc: 0.9074 - val_loss: 0.7084 - val_acc: 0.7556\n",
      "Epoch 14/20\n",
      "810/810 [==============================] - 14s 17ms/step - loss: 0.1960 - acc: 0.9259 - val_loss: 0.8578 - val_acc: 0.7556\n",
      "Epoch 15/20\n",
      "810/810 [==============================] - 13s 17ms/step - loss: 0.1735 - acc: 0.9346 - val_loss: 0.8462 - val_acc: 0.7222\n",
      "Epoch 16/20\n",
      "810/810 [==============================] - 13s 17ms/step - loss: 0.1183 - acc: 0.9543 - val_loss: 0.9244 - val_acc: 0.6889\n",
      "Epoch 17/20\n",
      "810/810 [==============================] - 13s 16ms/step - loss: 0.1117 - acc: 0.9605 - val_loss: 0.9338 - val_acc: 0.7111\n",
      "Epoch 18/20\n",
      "810/810 [==============================] - 14s 17ms/step - loss: 0.1564 - acc: 0.9494 - val_loss: 0.8128 - val_acc: 0.7556\n",
      "Epoch 19/20\n",
      "780/810 [===========================>..] - ETA: 0s - loss: 0.1119 - acc: 0.9577- ETA: 1s - loss: 0.1144 - acc:"
     ]
    }
   ],
   "source": [
    "first = Sequential()\n",
    "first.add(BatchNormalization(batch_size = 5,input_shape=(350,350,3)))\n",
    "first.add(Conv2D(8, kernel_size=(2,2), strides=(1, 1), padding='same', data_format='channels_last', activation='relu', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))\n",
    "\n",
    "first.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))\n",
    "\n",
    "first.add(Conv2D(8, kernel_size=(2,2), strides=(1, 1), padding='same', data_format='channels_last', activation='relu', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))\n",
    "\n",
    "first.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))\n",
    "first.add(Dropout(0.2))\n",
    "first.add(Conv2D(16, kernel_size=(2,2), strides=(1, 1), padding='same', data_format='channels_last', activation='relu', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))\n",
    "\n",
    "first.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))\n",
    "first.add(Dropout(0.2))\n",
    "first.add(Conv2D(32, kernel_size=(2,2), strides=(1, 1), padding='same', data_format='channels_last', activation='relu', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))\n",
    "\n",
    "first.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))\n",
    "first.add(Dropout(0.4))\n",
    "first.add(Flatten())\n",
    "\n",
    "#sgd = optimizers.SGD(lr=0.000001, decay=0.0005, momentum=0.95, nesterov=False)\n",
    "\n",
    "#first.compile(loss='mse', optimizer = sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "first.add(Dense(50, activation=\"relu\"))\n",
    "\n",
    "\n",
    "first.add(Dense(6,activation='softmax'))\n",
    "first.summary()\n",
    "\n",
    "first.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = first.fit(X_train, y1, batch_size=5, epochs=20, verbose=1, validation_split=0.1, shuffle=True)\n",
    "score = first.evaluate(X_test, y2, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
